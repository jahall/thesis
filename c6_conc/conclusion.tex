\chapter{C\lowercase{onclusions and} F\lowercase{uture} W\lowercase{ork}}
\section{Conclusions}
The central objective of this thesis was to provide methods to allow the incorporation of useful prior knowledge when designing a control policy for a system governed by unknown, or partially unknown, dynamics. The specific context which we considered was a probabilistic learning framework in which modelling uncertainty is interpreted as a posterior distribution over a space of dynamics functions. We were able to deal with distributions over spaces of functions using Gaussian processes. In addition to the central objective, we investigated the effect incorporating such information could have on learning and how our resulting learning algorithm compared with a standard control theoretic approach based on two case study examples. In these concluding remarks we shall summarise how these objectives have been achieved.

Prior knowledge about the dynamics of a given system often comes in the form of known, or approximate, relationships between a subset of the state variables. A common example of this is position-velocity dependencies. We outlined a novel and elegant method in which these known relationships could be incorporated into the probabilistic framework while uncertainty was still treated in a rigorous manner. To achieve this we considered how the predictions from multiple dynamics models could be combined to give an overall prediction of a state trajectory when a certain control policy was acting on the system. Experimental results on the pendulum swing-up task showed that including an approximate model for position-velocity relationships could help to avoid problems of finding local minima with some settings, in particular position reconstruction, but seemed to aggravate the problem with other settings, in particular velocity reconstruction. This was a surprising result and prompted us to apply caution before forcing our own assumptions onto a problem even if they seem sensible. Further results were obtained when we applied our algorithm to the problem of learning a control policy to balance a simulated robotic unicycle. Again, this pointed to the need for caution when forcing our beliefs into a model. One of the main advantages we found however was in terms of computational saving in learning where we get a learning speed up linearly proportional to the number of known state relationships we include.


The framework developed for multiple dynamics models, or augmented dynamics, we found could also be applied to the problem of learning a control policy to track a known reference signal or distribution over possible reference signals. Further, if the control policy had access to a preview horizon of the reference, this information could also be encoded. The capability to perform reference tracking was illustrated on a simulated inverted pendulum attached to a moving cart tracking a moving positional setpoint. We then demonstrated it on the simulated unicycle and showed that it was capable of learning how to track a complex spinning manoeuvre. 



The second problem we tackled in order to address the central objective was how we could encode the assumption that a discrete-time system was actually a sampled continuous-time system. In particular, how could we exploit any underlying structure in the continuous-time dynamics for prediction in discrete-time given only discrete data. We defined a set of Gaussian process priors over discrete-time dynamics based on a prior over the underlying continuous-time dynamics. The relationship exploited the approximate relationship between continuous and discrete-time systems given by the Runge-Kutta family of numerical integration methods. We demonstrated, on data sets obtained from the simulated pendulum, that our method can pick out structure in the continuous dynamics such as invariance, linear, additive and general nonlinear relationships between states. The resulting posterior distribution can exploit this information for improved predictive performance. A further useful feature of these priors are that we can combine non-uniformly sampled discrete data with continuous-time data, including local linear models.



In our final investigation, we took two case study examples in order to compare how the application of our learning algorithm compared with a standard control theoretic approach. This comparison was in largely in terms of discussing what expert knowledge was required of the user in order to implement either method.  In the case of the robotic unicycle we found that significant knowledge of the system was required of the user to even find an appropriate linear model on which to perform design. We noted that performance criteria: constraint satisfaction and dealing with modelling uncertainty often need to be addressed using the same set of design parameters in a standard control approach. When then compared the actual performance of a learned control policy and a policy derived using LQR. We found that, although the learned policy performed well in terms of the cost function over the finite prediction horizon, instead of balancing it spun the unicycle around in a tight orbit of the origin, which would eventually collapse after the prediction horizon had elapsed. This was also true of the evaporator study. Conversely, the learning approach provides quite a clear structure to separate out these issues leaving some relatively intuitive tuning parameters. One of the issues with applying learning on the evaporator was how to ensure that learning proceeded in a safe manner, this was addressed under the assumption that there was an existing control policy in place, even if that was simply a human operator. In addition we demonstrated the use of an additive RBF policy and showed that the learning approach was able to handle poor modelling assumptions in the form of wrongly estimated servomechanism lags.







\section{Future Work}
There are a number of avenues available to extend the work of this thesis. We begin with the framework of using multiple dynamics models as a way of incorporating prior knowledge into the learning algorithm. There is much scope for further investigation into the effects of incorporating approximate relationships between state variables. This has been touched upon in the setting of position-velocity relationships and lags on the control actions but looking at more general relationships could be fruitful. The overall aim would be to somehow determine when it would be beneficial to force such prior knowledge into the model and when it would be best to let the learning algorithm infer the information for itself.

We now turn to the class of priors over sampled continuous-time systems defined in Chapter 5. The benefit of using such a prior has been clearly demonstrated in terms of model likelihood and predictive performance when there is an underlying continuous-time structure in the system to be exploited. The next stage is to derive the predictive equations when the input is uncertain. We could therefore incorporate these priors into the full learning control algorithm and investigate what the benefits are in terms of learning control policies. The biggest current bottleneck to achieving this is the computational complexity of evaluating and training these priors. This would have to be reduced significantly in order for these priors to be of practical use in the context of learning control.



Finally, on a broader scale, we hope that the work of this thesis will be a step towards a ``recipe book" for different types of control problems. Specifically, we would like to determine which control or learning strategies would be most efficacious or suitable for a given control problem. The issue of what forms of prior knowledge would be most appropriate to incorporate into a given problem, as discussed above, would also play a significant role in this classification procedure. This work would help pull together the rich resources available from the fields of Machine Learning and Control in a way that could be useful for the practitioner.


%We believe the most pressing is to find a computationally efficient method to evaluate the sampled continuous time prior outlined in Chapter 5.  This is currently the biggest bottleneck in terms of the method gaining widespread acceptance. The next step would be to apply it in a learning control setting and investigate the performance improvements that can be made there in addition to the improvement in predictive performance we have observed in this work.

%Now consider the learning algorithm as a whole. An important avenue for making the algorithm more general is to address the intrinsic assumption of Gaussianity. While we have not found this to be restrictive in the problems we have considered, one can contrive scenarios where this assumption may be inappropriate. We believe the most promising approach to addressing this is to use a combination of sampling and analytic expressions. Although many questions remain open in this area.


%On a much broader scale, a useful research direction would be to find some way of classifying different control problems based on what method or algorithm would be most appropriate to use. In particular, to carry out this study based on the skill set of an average designer rather than the best possible performance that could be achieved. This will be useful to address in practice since many algorithms require a great deal of expert knowledge in order to obtain reasonable performance, whereas less elegant methods may be relatively easy to tune by the layperson.